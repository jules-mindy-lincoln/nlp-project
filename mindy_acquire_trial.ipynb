{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ff1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "import requests\n",
    "import prepare\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d555d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 290 entries, 0 to 391\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   repo             290 non-null    object\n",
      " 1   language         290 non-null    object\n",
      " 2   readme_contents  290 non-null    object\n",
      " 3   stemmed          290 non-null    object\n",
      " 4   lemmatized       290 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 13.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = prepare.wrangle_data()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1dcd829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo</th>\n",
       "      <th>language</th>\n",
       "      <th>readme_contents</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dreamingechoes/awesome-mental-health</td>\n",
       "      <td>HTML</td>\n",
       "      <td>\\n&lt;p align=\"center\"&gt;&lt;img src=\"./media/logo.png...</td>\n",
       "      <td>aboutsparkl curat list awesom articl websit re...</td>\n",
       "      <td>aboutsparkles curated list awesome article web...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flaque/quirk</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>ðŸš§ðŸš§ðŸš§\\n\\n**Quirk is no longer being maintained.*...</td>\n",
       "      <td>quirk longer maintainedquirk start littl thing...</td>\n",
       "      <td>quirk longer maintainedquirk started little th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sepandhaghighi/nafas</td>\n",
       "      <td>Python</td>\n",
       "      <td>&lt;div align=\"center\"&gt;\\n&lt;img src=\"https://github...</td>\n",
       "      <td>nafa &amp;#9; tabl content overview instal usag is...</td>\n",
       "      <td>nafas &amp;#9; table content overview installation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>youarerad/youareradweb</td>\n",
       "      <td>TypeScript</td>\n",
       "      <td>&lt;div align=\"center\"&gt;&lt;img src=\"https://res.clou...</td>\n",
       "      <td>welcom rise disord ' websit repow nonprofit co...</td>\n",
       "      <td>welcome rise disorder ' website repowe nonprof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OSMIHelp/osmi-survey-graph</td>\n",
       "      <td>PHP</td>\n",
       "      <td># 2016 OSMI Survey Graph\\n\\n## Installing and ...</td>\n",
       "      <td>2016 osmi survey graph instal run copi envexam...</td>\n",
       "      <td>2016 osmi survey graph installing running copy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   repo    language  \\\n",
       "0  dreamingechoes/awesome-mental-health        HTML   \n",
       "1                          Flaque/quirk  TypeScript   \n",
       "4                  sepandhaghighi/nafas      Python   \n",
       "5                youarerad/youareradweb  TypeScript   \n",
       "7            OSMIHelp/osmi-survey-graph         PHP   \n",
       "\n",
       "                                     readme_contents  \\\n",
       "0  \\n<p align=\"center\"><img src=\"./media/logo.png...   \n",
       "1  ðŸš§ðŸš§ðŸš§\\n\\n**Quirk is no longer being maintained.*...   \n",
       "4  <div align=\"center\">\\n<img src=\"https://github...   \n",
       "5  <div align=\"center\"><img src=\"https://res.clou...   \n",
       "7  # 2016 OSMI Survey Graph\\n\\n## Installing and ...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  aboutsparkl curat list awesom articl websit re...   \n",
       "1  quirk longer maintainedquirk start littl thing...   \n",
       "4  nafa &#9; tabl content overview instal usag is...   \n",
       "5  welcom rise disord ' websit repow nonprofit co...   \n",
       "7  2016 osmi survey graph instal run copi envexam...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  aboutsparkles curated list awesome article web...  \n",
       "1  quirk longer maintainedquirk started little th...  \n",
       "4  nafas &#9; table content overview installation...  \n",
       "5  welcome rise disorder ' website repowe nonprof...  \n",
       "7  2016 osmi survey graph installing running copy...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d0039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import time\n",
    "# import csv\n",
    "# import json\n",
    "# from typing import Dict, List, Optional, Union, cast\n",
    "# import requests\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# from env import github_token, github_username\n",
    "\n",
    "# # Check if repo csv exists\n",
    "# #if not os.path.isfile(\"repo.csv\"):\n",
    "    \n",
    "    \n",
    "# lang_list = ['JavaScript', 'HTML', 'Java', 'Python', 'CSS', 'C#', 'PHP', 'TypeScript', 'R']\n",
    "# #     page_num = [1-23]\n",
    "# repos = []\n",
    "\n",
    "# #     for page in page_num:\n",
    "# for i in range(1,41):\n",
    "\n",
    "# #             url = 'https://github.com/search?l={lang}&p={i}&q=mental+health&type=repositories'\n",
    "#     url = f\"https://github.com/search?p={i}&q=%23mental-health&type=Repositories\"\n",
    "#     while True:\n",
    "#         response = requests.get(url)\n",
    "#         if response.ok:\n",
    "#             print(True)\n",
    "#             break\n",
    "#         else:\n",
    "#             print('sleeping')\n",
    "#             time.sleep(20)\n",
    "            \n",
    "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "#     for element in soup.find_all('a', class_='v-align-middle'):\n",
    "#         repos.append(element.text)\n",
    "\n",
    "# #         time.sleep(10)\n",
    "\n",
    "# with open('repo.csv', 'w') as createfile:\n",
    "#     wr = csv.writer(createfile, quoting=csv.QUOTE_ALL)\n",
    "#     wr.writerow(repos)\n",
    "#     results = []\n",
    "# with open('repo.csv', newline='') as inputfile:\n",
    "#     results = list(csv.reader(inputfile))\n",
    "\n",
    "#     REPOS = [item for sublist in results for item in sublist]\n",
    "#     headers = {\"Authorization\": f\"token {github_token}\", \"User-Agent\": github_username}\n",
    "\n",
    "# if headers[\"Authorization\"] == \"token \" or headers[\"User-Agent\"] == \"\":\n",
    "#     raise Exception(\n",
    "#     \"You need to follow the instructions marked TODO in this script before trying to use it\"\n",
    "# )\n",
    "\n",
    "\n",
    "# def github_api_request(url: str) -> Union[List, Dict]:\n",
    "#     response = requests.get(url, headers=headers)\n",
    "#     response_data = response.json()\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(\n",
    "#             f\"Error response from github api! status code: {response.status_code}, \"\n",
    "#             f\"response: {json.dumps(response_data)}\"\n",
    "#         )\n",
    "#     return response_data\n",
    "\n",
    "\n",
    "# def get_repo_language(repo: str) -> str:\n",
    "#     url = f\"https://api.github.com/repos/{repo}\"\n",
    "#     repo_info = github_api_request(url)\n",
    "#     if type(repo_info) is dict:\n",
    "#         repo_info = cast(Dict, repo_info)\n",
    "#         return repo_info.get(\"language\", None)\n",
    "#     raise Exception(\n",
    "#         f\"Expecting a dictionary response from {url}, instead got {json.dumps(repo_info)}\"\n",
    "#     )\n",
    "    \n",
    "# def get_repo_contents(repo: str) -> List[Dict[str, str]]:\n",
    "#     url = f\"https://api.github.com/repos/{repo}/contents/\"\n",
    "#     contents = github_api_request(url)\n",
    "#     if type(contents) is list:\n",
    "#         contents = cast(List, contents)\n",
    "#         return contents\n",
    "#     raise Exception(\n",
    "#         f\"Expecting a list response from {url}, instead got {json.dumps(contents)}\"\n",
    "#     )\n",
    "\n",
    "# def get_readme_download_url(files: List[Dict[str, str]]) -> str:\n",
    "#     \"\"\"\n",
    "#     Takes in a response from the github api that lists the files in a repo and\n",
    "#     returns the url that can be used to download the repo's README file.\n",
    "#     \"\"\"\n",
    "#     for file in files:\n",
    "#         if file[\"name\"].lower().startswith(\"readme\"):\n",
    "#             return file[\"download_url\"]\n",
    "#     return \"\"\n",
    "\n",
    "# def process_repo(repo: str) -> Dict[str, str]:\n",
    "#     \"\"\"\n",
    "#     Takes a repo name like \"gocodeup/codeup-setup-script\" and returns a\n",
    "#     dictionary with the language of the repo and the readme contents.\n",
    "#     \"\"\"\n",
    "#     contents = get_repo_contents(repo)\n",
    "#     readme_download_url = get_readme_download_url(contents)\n",
    "#     if readme_download_url == \"\":\n",
    "#         readme_contents = None\n",
    "#     else:\n",
    "#         readme_contents = requests.get(readme_download_url).text\n",
    "#     return {\n",
    "#         \"repo\": repo,\n",
    "#         \"language\": get_repo_language(repo),\n",
    "#         \"readme_contents\": readme_contents,\n",
    "#     }\n",
    "\n",
    "# def scrape_github_data() -> List[Dict[str, str]]:\n",
    "#     \"\"\"\n",
    "#     Loop through all of the repos and process them. Returns the processed data.\n",
    "#     \"\"\"\n",
    "#     return [process_repo(repo) for repo in REPOS]\n",
    "\n",
    "\n",
    "# #if __name__ == \"__main__\":\n",
    "# data = scrape_github_data()\n",
    "# json.dump(data, open(\"data.json\", \"w\"), indent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0649541",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af26bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2def53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df = df[df['language'].map(df['language'].value_counts()) >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1651f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04f69b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef92b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_original'] = df['readme_contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af997a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    string = string.lower()\n",
    "    string = (unicodedata.normalize('NFKD', string)\n",
    "                         .encode('ascii', 'ignore')\n",
    "                         .decode('utf-8', 'ignore')\n",
    "             )\n",
    "    string = re.sub(r\"[^a-z0-9'\\s]\", '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beada2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents'] = df['readme_contents'].apply(basic_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd4934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(string):\n",
    "    string = re.sub(r'<[^>]*>', '', string)\n",
    "    string = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\", '', string)\n",
    "    string = re.sub(r'\\n', '', string)\n",
    "    string = re.sub(r'\\s\\s', '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25419255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents'] = df['readme_contents'].apply(clean_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e923d8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    return tokenizer.tokenize(string, return_str=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6746f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents'] = df['readme_contents'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4ea7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ad92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents'] = df['readme_contents'].apply(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d74737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d552fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents'] = df['readme_contents'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53beb46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    for word in extra_words:\n",
    "        stopword_list.append(word)\n",
    "    \n",
    "    for word in exclude_words:\n",
    "        stopword_list.remove(word)\n",
    "        \n",
    "    words = string.split()\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa71cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents'] = df['readme_contents'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6626eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['readme_contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72130c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_readme_data(df, column):\n",
    "    clean_tokens = (df[column].apply(clean_html)\n",
    "                              .apply(basic_clean)\n",
    "                              .apply(tokenize)\n",
    "                              .apply(remove_stopwords)\n",
    "                   )\n",
    "    \n",
    "    for token in clean_tokens:\n",
    "        token = ' '.join(token).split()\n",
    "    \n",
    "    df['stemmed'] = clean_tokens.apply(stem)\n",
    "    df['lemmatized'] = clean_tokens.apply(lemmatize)\n",
    "    return df\n",
    "\n",
    "def wrangle_data():\n",
    "    data = pd.read_json('data.json')\n",
    "    return prepare_readme_data(data, 'readme_contents')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
